<html>
<head>
<title> 5 Risks of AI</title>
</head>
<style>
body {
  background-image: url('background.jpg');
  background-repeat: no-repeat;
  background-attachment: fixed;
  background-size: 100% 100%;
}
</style>
<body>

<h1>&#9658; 5 <strong style="color:Tomato;font-size:110%;"> Risks </strong> of artificial intelligence</h1>

<p>It is therefore sensible to have a close look at a potentially powerful technology such as artificial intelligence: this should include both the positive and the less positive sides. Here we go.
</p>
<b><p style="color:Tomato;font-size:160%;">1. A lack of transparency
</b>
<p>Many AI systems were built with so-called neural networks serving as the engine; these are complex interconnected node systems. However, these systems are less capable of indicating their ‘motivation’ for decisions. You only see the input and the output. The system is far too complex. Nevertheless, where military or medical decisions are involved, it is important to be able to trace back the specific data that resulted in specific decisions. What underlying thought or reasoning resulted in the output? What data was used to train the model? How does the model ‘think’? We are currently generally in the dark about this.
<b><p style="color:Tomato;font-size:160%;">2. Biased algorithms </b>
<p>When we feed our algorithms data sets that contain biased data, the system will logically confirm our biases. There are currently many examples of systems that disadvantage ethnic minorities to a greater degree than is the case with the white population. After all, when a system is fed discriminatory data, it will produce this type of data. Garbage in, garbage out. And because the output is from a computer, the answer will tend to be assumed to be true. (This is based on the so-called automation bias, which is the human tendency to take suggestions from “automated decision-making systems” more seriously and ignore contradictory data created by people, even if it is correct). And when discriminatory systems are fed new discriminatory data (because that is what the computer says) it turns into a self-fulfilling prophecy. And remember, biases are often a blind spot.
<b><p style="color:Tomato;font-size:160%;">3. Liability for actions </b>
<p>A great deal is still unclear about the legal aspects of systems that become increasingly smart. What is the situation in terms of liability when the AI system makes an error? Do we judge this like we would judge a human? Who is responsible in a scenario in which systems become self-learning and autonomous to a greater extent? Can a company still be held accountable for an algorithm that has learned by itself and subsequently determines its own course, and which, based on massive amounts of data, has drawn its own conclusions in order to reach specific decisions? Do we accept an error margin of AI machines, even if this sometimes has fatal consequences?
<b><p style="color:Tomato;font-size:160%;">4. Too big a mandate </b>
<p>The more smart systems we use, the more we will run into the issue of scope. What is the extent of the mandate we give our smart virtual assistants? What are and aren’t they allowed to decide for us? Do we stretch the autonomy of smart systems ever further or should we stay in control of this at any cost, such as is preferred by the European Union? What do and don’t we allow smart systems to determine and implement without human intervention? And should a preview function perhaps be installed in smart AI systems as standard? The risk exists that we transfer too much autonomy, without the technology and preconditions being fully developed, and without us remaining aware over time where we have outsourced the relevant tasks and for what reason. Indeed, there is a risk that we increasingly end up in a world we no longer understand. We must not lose sight of our interpersonal empathy and solidarity as there is a real risk we leave difficult decisions (e.g. employment dismissal) to ‘smart’ machines too easily because we consider this to be too difficult ourselves.
<b><p style="color:Tomato;font-size:160%;">5. Too little privacy </b>
<p>We create 2.5 quintillion bytes of data each day (which is 2.5 million terabytes, where 1 terabyte is 1,000 gigabyte). Of all digital data in the world, 90 per cent has been created in the last two years. A company requires substantial amounts of pure data to allow for the proper functioning of its smart systems. Apart from high-quality algorithms, the strength of an AI system also lies in having high-quality data sets at one’s disposal. Companies that are involved in artificial intelligence are increasingly turning into Greedy Gus when it comes to our data: it is never enough and anything is justified to achieve even better results. The risk, for example, is that companies create an ever more clearly defined profile of us with ever greater precision, and that these resources are also used for political purposes.
<p>
The result is that our privacy is being eroded. However, when we subsequently protect our personal privacy, said companies will simply use similar target groups; people that look very much like us. And our data is resold en masse, with an increasing loss of awareness as to who receives it or for what purposes it is being used. Data is the lubricating oil of AI systems and our privacy is at stake in any event.
<p>&#11013;<button onclick="document.location='Mainpage.htm'"> Back Home </button>
</body>
</html>
